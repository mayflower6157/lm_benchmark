{
  "results": {
    "mmlu_stem": {
      "acc,none": 0.6092610212496036,
      "acc_stderr,none": 0.008445585573222999,
      "alias": "stem"
    },
    "mmlu_abstract_algebra": {
      "alias": " - abstract_algebra",
      "acc,none": 0.46,
      "acc_stderr,none": 0.05009082659620332
    },
    "mmlu_anatomy": {
      "alias": " - anatomy",
      "acc,none": 0.6666666666666666,
      "acc_stderr,none": 0.04072314811876837
    },
    "mmlu_astronomy": {
      "alias": " - astronomy",
      "acc,none": 0.743421052631579,
      "acc_stderr,none": 0.0355418036802569
    },
    "mmlu_college_biology": {
      "alias": " - college_biology",
      "acc,none": 0.7361111111111112,
      "acc_stderr,none": 0.03685651095897532
    },
    "mmlu_college_chemistry": {
      "alias": " - college_chemistry",
      "acc,none": 0.49,
      "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_college_computer_science": {
      "alias": " - college_computer_science",
      "acc,none": 0.59,
      "acc_stderr,none": 0.04943110704237102
    },
    "mmlu_college_mathematics": {
      "alias": " - college_mathematics",
      "acc,none": 0.31,
      "acc_stderr,none": 0.04648231987117316
    },
    "mmlu_college_physics": {
      "alias": " - college_physics",
      "acc,none": 0.49019607843137253,
      "acc_stderr,none": 0.04974229460422817
    },
    "mmlu_computer_security": {
      "alias": " - computer_security",
      "acc,none": 0.72,
      "acc_stderr,none": 0.045126085985421296
    },
    "mmlu_conceptual_physics": {
      "alias": " - conceptual_physics",
      "acc,none": 0.6425531914893617,
      "acc_stderr,none": 0.031329417894764254
    },
    "mmlu_electrical_engineering": {
      "alias": " - electrical_engineering",
      "acc,none": 0.6344827586206897,
      "acc_stderr,none": 0.04013124195424385
    },
    "mmlu_elementary_mathematics": {
      "alias": " - elementary_mathematics",
      "acc,none": 0.6058201058201058,
      "acc_stderr,none": 0.025167982333894143
    },
    "mmlu_high_school_biology": {
      "alias": " - high_school_biology",
      "acc,none": 0.8193548387096774,
      "acc_stderr,none": 0.021886178567172565
    },
    "mmlu_high_school_chemistry": {
      "alias": " - high_school_chemistry",
      "acc,none": 0.5812807881773399,
      "acc_stderr,none": 0.03471192860518469
    },
    "mmlu_high_school_computer_science": {
      "alias": " - high_school_computer_science",
      "acc,none": 0.75,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_high_school_mathematics": {
      "alias": " - high_school_mathematics",
      "acc,none": 0.4962962962962963,
      "acc_stderr,none": 0.03048470166508437
    },
    "mmlu_high_school_physics": {
      "alias": " - high_school_physics",
      "acc,none": 0.45695364238410596,
      "acc_stderr,none": 0.04067325174247443
    },
    "mmlu_high_school_statistics": {
      "alias": " - high_school_statistics",
      "acc,none": 0.5972222222222222,
      "acc_stderr,none": 0.033448873829978666
    },
    "mmlu_machine_learning": {
      "alias": " - machine_learning",
      "acc,none": 0.48214285714285715,
      "acc_stderr,none": 0.047427623612430116
    }
  },
  "groups": {
    "mmlu_stem": {
      "acc,none": 0.6092610212496036,
      "acc_stderr,none": 0.008445585573222999,
      "alias": "stem"
    }
  },
  "group_subtasks": {
    "mmlu_stem": [
      "mmlu_abstract_algebra",
      "mmlu_high_school_chemistry",
      "mmlu_conceptual_physics",
      "mmlu_high_school_physics",
      "mmlu_high_school_statistics",
      "mmlu_college_physics",
      "mmlu_high_school_biology",
      "mmlu_astronomy",
      "mmlu_college_computer_science",
      "mmlu_electrical_engineering",
      "mmlu_elementary_mathematics",
      "mmlu_high_school_mathematics",
      "mmlu_college_mathematics",
      "mmlu_college_biology",
      "mmlu_anatomy",
      "mmlu_high_school_computer_science",
      "mmlu_college_chemistry",
      "mmlu_machine_learning",
      "mmlu_computer_security"
    ]
  },
  "configs": {
    "mmlu_abstract_algebra": {
      "task": "mmlu_abstract_algebra",
      "task_alias": "abstract_algebra",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "abstract_algebra",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_anatomy": {
      "task": "mmlu_anatomy",
      "task_alias": "anatomy",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "anatomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_astronomy": {
      "task": "mmlu_astronomy",
      "task_alias": "astronomy",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "astronomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_college_biology": {
      "task": "mmlu_college_biology",
      "task_alias": "college_biology",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_college_chemistry": {
      "task": "mmlu_college_chemistry",
      "task_alias": "college_chemistry",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_college_computer_science": {
      "task": "mmlu_college_computer_science",
      "task_alias": "college_computer_science",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_college_mathematics": {
      "task": "mmlu_college_mathematics",
      "task_alias": "college_mathematics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_college_physics": {
      "task": "mmlu_college_physics",
      "task_alias": "college_physics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_computer_security": {
      "task": "mmlu_computer_security",
      "task_alias": "computer_security",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "computer_security",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_conceptual_physics": {
      "task": "mmlu_conceptual_physics",
      "task_alias": "conceptual_physics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "conceptual_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_electrical_engineering": {
      "task": "mmlu_electrical_engineering",
      "task_alias": "electrical_engineering",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "electrical_engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_elementary_mathematics": {
      "task": "mmlu_elementary_mathematics",
      "task_alias": "elementary_mathematics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "elementary_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_high_school_biology": {
      "task": "mmlu_high_school_biology",
      "task_alias": "high_school_biology",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_high_school_chemistry": {
      "task": "mmlu_high_school_chemistry",
      "task_alias": "high_school_chemistry",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_high_school_computer_science": {
      "task": "mmlu_high_school_computer_science",
      "task_alias": "high_school_computer_science",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_high_school_mathematics": {
      "task": "mmlu_high_school_mathematics",
      "task_alias": "high_school_mathematics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_high_school_physics": {
      "task": "mmlu_high_school_physics",
      "task_alias": "high_school_physics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_high_school_statistics": {
      "task": "mmlu_high_school_statistics",
      "task_alias": "high_school_statistics",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_statistics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    },
    "mmlu_machine_learning": {
      "task": "mmlu_machine_learning",
      "task_alias": "machine_learning",
      "tag": "mmlu_stem_tasks",
      "dataset_path": "cais/mmlu",
      "dataset_name": "machine_learning",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "Qwen/Qwen2.5-3B-Instruct",
        "dtype": "bfloat16",
        "attn_implementation": "flash_attention_2"
      }
    }
  },
  "versions": {
    "mmlu_abstract_algebra": 1.0,
    "mmlu_anatomy": 1.0,
    "mmlu_astronomy": 1.0,
    "mmlu_college_biology": 1.0,
    "mmlu_college_chemistry": 1.0,
    "mmlu_college_computer_science": 1.0,
    "mmlu_college_mathematics": 1.0,
    "mmlu_college_physics": 1.0,
    "mmlu_computer_security": 1.0,
    "mmlu_conceptual_physics": 1.0,
    "mmlu_electrical_engineering": 1.0,
    "mmlu_elementary_mathematics": 1.0,
    "mmlu_high_school_biology": 1.0,
    "mmlu_high_school_chemistry": 1.0,
    "mmlu_high_school_computer_science": 1.0,
    "mmlu_high_school_mathematics": 1.0,
    "mmlu_high_school_physics": 1.0,
    "mmlu_high_school_statistics": 1.0,
    "mmlu_machine_learning": 1.0,
    "mmlu_stem": 2
  },
  "n-shot": {
    "mmlu_abstract_algebra": 0,
    "mmlu_anatomy": 0,
    "mmlu_astronomy": 0,
    "mmlu_college_biology": 0,
    "mmlu_college_chemistry": 0,
    "mmlu_college_computer_science": 0,
    "mmlu_college_mathematics": 0,
    "mmlu_college_physics": 0,
    "mmlu_computer_security": 0,
    "mmlu_conceptual_physics": 0,
    "mmlu_electrical_engineering": 0,
    "mmlu_elementary_mathematics": 0,
    "mmlu_high_school_biology": 0,
    "mmlu_high_school_chemistry": 0,
    "mmlu_high_school_computer_science": 0,
    "mmlu_high_school_mathematics": 0,
    "mmlu_high_school_physics": 0,
    "mmlu_high_school_statistics": 0,
    "mmlu_machine_learning": 0
  },
  "higher_is_better": {
    "mmlu_abstract_algebra": {
      "acc": true
    },
    "mmlu_anatomy": {
      "acc": true
    },
    "mmlu_astronomy": {
      "acc": true
    },
    "mmlu_college_biology": {
      "acc": true
    },
    "mmlu_college_chemistry": {
      "acc": true
    },
    "mmlu_college_computer_science": {
      "acc": true
    },
    "mmlu_college_mathematics": {
      "acc": true
    },
    "mmlu_college_physics": {
      "acc": true
    },
    "mmlu_computer_security": {
      "acc": true
    },
    "mmlu_conceptual_physics": {
      "acc": true
    },
    "mmlu_electrical_engineering": {
      "acc": true
    },
    "mmlu_elementary_mathematics": {
      "acc": true
    },
    "mmlu_high_school_biology": {
      "acc": true
    },
    "mmlu_high_school_chemistry": {
      "acc": true
    },
    "mmlu_high_school_computer_science": {
      "acc": true
    },
    "mmlu_high_school_mathematics": {
      "acc": true
    },
    "mmlu_high_school_physics": {
      "acc": true
    },
    "mmlu_high_school_statistics": {
      "acc": true
    },
    "mmlu_machine_learning": {
      "acc": true
    },
    "mmlu_stem": {
      "acc": true
    }
  },
  "n-samples": {
    "mmlu_abstract_algebra": {
      "original": 100,
      "effective": 100
    },
    "mmlu_high_school_chemistry": {
      "original": 203,
      "effective": 203
    },
    "mmlu_conceptual_physics": {
      "original": 235,
      "effective": 235
    },
    "mmlu_high_school_physics": {
      "original": 151,
      "effective": 151
    },
    "mmlu_high_school_statistics": {
      "original": 216,
      "effective": 216
    },
    "mmlu_college_physics": {
      "original": 102,
      "effective": 102
    },
    "mmlu_high_school_biology": {
      "original": 310,
      "effective": 310
    },
    "mmlu_astronomy": {
      "original": 152,
      "effective": 152
    },
    "mmlu_college_computer_science": {
      "original": 100,
      "effective": 100
    },
    "mmlu_electrical_engineering": {
      "original": 145,
      "effective": 145
    },
    "mmlu_elementary_mathematics": {
      "original": 378,
      "effective": 378
    },
    "mmlu_high_school_mathematics": {
      "original": 270,
      "effective": 270
    },
    "mmlu_college_mathematics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_college_biology": {
      "original": 144,
      "effective": 144
    },
    "mmlu_anatomy": {
      "original": 135,
      "effective": 135
    },
    "mmlu_high_school_computer_science": {
      "original": 100,
      "effective": 100
    },
    "mmlu_college_chemistry": {
      "original": 100,
      "effective": 100
    },
    "mmlu_machine_learning": {
      "original": 112,
      "effective": 112
    },
    "mmlu_computer_security": {
      "original": 100,
      "effective": 100
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=Qwen/Qwen2.5-3B-Instruct,dtype=bfloat16,attn_implementation=flash_attention_2",
    "model_num_parameters": 3085938688,
    "model_dtype": "torch.bfloat16",
    "model_revision": "main",
    "model_sha": "aa8e72537993ba99e69dfaafa59ed015b17504d1",
    "batch_size": "auto",
    "batch_sizes": [
      32
    ],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": null,
  "date": 1758792890.315345,
  "pretty_env_info": "PyTorch version: 2.8.0+cu128\nIs debug build: False\nCUDA used to build PyTorch: 12.8\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 24.04.3 LTS (x86_64)\nGCC version: Could not collect\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.39\n\nPython version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-6.14.0-1015-gcp-x86_64-with-glibc2.39\nIs CUDA available: True\nCUDA runtime version: 12.0.140\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA L4\nNvidia driver version: 580.65.06\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                            x86_64\nCPU op-mode(s):                          32-bit, 64-bit\nAddress sizes:                           46 bits physical, 48 bits virtual\nByte Order:                              Little Endian\nCPU(s):                                  8\nOn-line CPU(s) list:                     0-7\nVendor ID:                               GenuineIntel\nModel name:                              Intel(R) Xeon(R) CPU @ 2.20GHz\nCPU family:                              6\nModel:                                   85\nThread(s) per core:                      2\nCore(s) per socket:                      4\nSocket(s):                               1\nStepping:                                7\nBogoMIPS:                                4400.34\nFlags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\nHypervisor vendor:                       KVM\nVirtualization type:                     full\nL1d cache:                               128 KiB (4 instances)\nL1i cache:                               128 KiB (4 instances)\nL2 cache:                                4 MiB (4 instances)\nL3 cache:                                38.5 MiB (1 instance)\nNUMA node(s):                            1\nNUMA node0 CPU(s):                       0-7\nVulnerability Gather data sampling:      Not affected\nVulnerability Ghostwrite:                Not affected\nVulnerability Indirect target selection: Mitigation; Aligned branch/return thunks\nVulnerability Itlb multihit:             Not affected\nVulnerability L1tf:                      Not affected\nVulnerability Mds:                       Not affected\nVulnerability Meltdown:                  Not affected\nVulnerability Mmio stale data:           Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Reg file data sampling:    Not affected\nVulnerability Retbleed:                  Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:      Not affected\nVulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                     Not affected\nVulnerability Tsx async abort:           Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n\nVersions of relevant libraries:\n[pip3] numpy==2.1.2\n[pip3] nvidia-cublas-cu12==12.8.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.8.90\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.93\n[pip3] nvidia-cuda-runtime-cu12==12.8.90\n[pip3] nvidia-cudnn-cu12==9.10.2.21\n[pip3] nvidia-cufft-cu12==11.3.3.83\n[pip3] nvidia-curand-cu12==10.3.9.90\n[pip3] nvidia-cusolver-cu12==11.7.3.90\n[pip3] nvidia-cusparse-cu12==12.5.8.93\n[pip3] nvidia-cusparselt-cu12==0.7.1\n[pip3] nvidia-nccl-cu12==2.27.3\n[pip3] nvidia-nvjitlink-cu12==12.8.93\n[pip3] nvidia-nvtx-cu12==12.8.90\n[pip3] torch==2.8.0+cu128\n[pip3] torchaudio==2.8.0+cu128\n[pip3] torchvision==0.23.0+cu128\n[pip3] triton==3.4.0\n[conda] numpy                            2.1.2            pypi_0              pypi\n[conda] nvidia-cublas-cu12               12.8.4.1         pypi_0              pypi\n[conda] nvidia-cuda-cupti-cu12           12.8.90          pypi_0              pypi\n[conda] nvidia-cuda-nvrtc-cu12           12.8.93          pypi_0              pypi\n[conda] nvidia-cuda-runtime-cu12         12.8.90          pypi_0              pypi\n[conda] nvidia-cudnn-cu12                9.10.2.21        pypi_0              pypi\n[conda] nvidia-cufft-cu12                11.3.3.83        pypi_0              pypi\n[conda] nvidia-curand-cu12               10.3.9.90        pypi_0              pypi\n[conda] nvidia-cusolver-cu12             11.7.3.90        pypi_0              pypi\n[conda] nvidia-cusparse-cu12             12.5.8.93        pypi_0              pypi\n[conda] nvidia-cusparselt-cu12           0.7.1            pypi_0              pypi\n[conda] nvidia-nccl-cu12                 2.27.3           pypi_0              pypi\n[conda] nvidia-nvjitlink-cu12            12.8.93          pypi_0              pypi\n[conda] nvidia-nvtx-cu12                 12.8.90          pypi_0              pypi\n[conda] torch                            2.8.0+cu128      pypi_0              pypi\n[conda] torchaudio                       2.8.0+cu128      pypi_0              pypi\n[conda] torchvision                      0.23.0+cu128     pypi_0              pypi\n[conda] triton                           3.4.0            pypi_0              pypi",
  "transformers_version": "4.56.2",
  "lm_eval_version": "0.4.9.1",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<|endoftext|>",
    "151643"
  ],
  "tokenizer_eos_token": [
    "<|im_end|>",
    "151645"
  ],
  "tokenizer_bos_token": [
    null,
    "None"
  ],
  "eot_token_id": 151645,
  "max_length": 32768,
  "task_hashes": {
    "mmlu_abstract_algebra": "019c53bb7725c435b6977919f6e4a0043f6045691070942190fe4e0257b6e1e4",
    "mmlu_high_school_chemistry": "1c7e3e5bffefd467481de9fb6425ec50eca053f9fce3b25af745ff886195176b",
    "mmlu_conceptual_physics": "ab3e1ecbb255ddc5c9ce70494d102bda7e259eb20596633235a42ca3d635239b",
    "mmlu_high_school_physics": "59513856cfc584e2815f43814216c8143f9c8866599ed8aaf7d53eec6ce308e9",
    "mmlu_high_school_statistics": "d46af02553938b20e9bce032a6ad424a0d56ae6e7784d0a351a96185695653f0",
    "mmlu_college_physics": "3f3da5b2a15744fd5445d372a816c3b07433b0ea50cb9e7fc8e08a8b2b2b962b",
    "mmlu_high_school_biology": "18ee3f74ce477d1ee3492951cfae846b1903dfcc4d107227ffaa6e305681a20f",
    "mmlu_astronomy": "c9eca6773bb6f58214e51f833bfd88e5eafcaa0c05d0a4c2ee3e9bbed1272002",
    "mmlu_college_computer_science": "44cc706099add4f2fa3d3903e33447378c38401a9b22e738008aef4db99ca7ae",
    "mmlu_electrical_engineering": "fd6ef46bf380068043ad0568d1987c5485397a06d582f3f546bf2bea6cc02f3a",
    "mmlu_elementary_mathematics": "5f96932b45fc8d0ea0e09c979e7a0290505fb53fdb647624ad00ca162a2a7c50",
    "mmlu_high_school_mathematics": "fcb250f2c0a888667054bdaa209b5c2b677ecc9c1ac81fa8b8dce87a05dbc3d7",
    "mmlu_college_mathematics": "f1a2766207148367dedfa3e2961fc69de59078cfbc9631210b38068c0df8bbd7",
    "mmlu_college_biology": "d983837a4ac4327e74ff7f131eda1f0c23f6c9f2a1088e3a5162c6ede31605d5",
    "mmlu_anatomy": "8a394ba6aa4d3366637e72da67c7d4c0286d47cb371a4f4a9814259be8bbe3ad",
    "mmlu_high_school_computer_science": "34a7f3d2bbe6a0dc39d03973d87f9053076ccfbd7ea7ab40dca7073b68640db7",
    "mmlu_college_chemistry": "9d6d9332909abd7956faabfd895b7bd46a1085f65f31678e8f3535fee315a29a",
    "mmlu_machine_learning": "4aa26a0049db413da3860533cc38acdf747bafd4849f6f6fc9f58028bb8b4cc6",
    "mmlu_computer_security": "9d94057a3894877d08645c17c769a104d28a2ed4249de8865d23d46953b15545"
  },
  "model_source": "hf",
  "model_name": "Qwen/Qwen2.5-3B-Instruct",
  "model_name_sanitized": "Qwen__Qwen2.5-3B-Instruct",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": null,
  "chat_template_sha": null,
  "start_time": 12438.285254234,
  "end_time": 12635.17387798,
  "total_evaluation_time_seconds": "196.8886237459992"
}